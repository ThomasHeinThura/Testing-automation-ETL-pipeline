{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* you need to make model pipline and testing the model with it (error)\n",
    "\n",
    "* Saving confusion matrix in MLFLOW (Good)\n",
    " \n",
    "* Need to add sqlite backend and automatic regristary the model\n",
    "  \n",
    "* add experiment name and add vresion\n",
    "\n",
    "* add automatic select the best model and predict the model with it with new unseen dataset.\n",
    "  \n",
    "* Train test val spliiting in dataset\n",
    "  \n",
    "* Make result dataframe and output file\n",
    "\n",
    "* MOve all parameters to main file and remove from others\n",
    "\n",
    "* make argv to make sure what model need to be train. \n",
    "\n",
    "* print to logger\n",
    "  \n",
    "* Make new dataset for testing deploy model\n",
    "\n",
    "* Deployment to flask and docker\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/25 11:15:02 INFO mlflow.tracking.fluent: Experiment with name 'my-experiment-1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/hanlinn/Projects/00_Summarization_CI_CD/test_CI_CD/src/model/mlruns/414775689427378622', creation_time=1682397902951, experiment_id='414775689427378622', last_update_time=1682397902951, lifecycle_stage='active', name='my-experiment-1', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"my-experiment-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='file:///home/hanlinn/Projects/00_Summarization_CI_CD/test_CI_CD/src/model/mlruns/414775689427378622', creation_time=1682397902951, experiment_id='414775689427378622', last_update_time=1682397902951, lifecycle_stage='active', name='my-experiment-1', tags={}>,\n",
       " <Experiment: artifact_location='file:///home/hanlinn/Projects/00_Summarization_CI_CD/ML_test/src/model/mlruns/0', creation_time=1682176918041, experiment_id='0', last_update_time=1682176918041, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.search_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mlflow\u001b[39m.\u001b[39;49mget_experiment(\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.8/site-packages/mlflow/tracking/fluent.py:1023\u001b[0m, in \u001b[0;36mget_experiment\u001b[0;34m(experiment_id)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_experiment\u001b[39m(experiment_id: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Experiment:\n\u001b[1;32m    996\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[39m    Retrieve an experiment by experiment_id from the backend store\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[39m        Creation timestamp: 1662004217511\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1023\u001b[0m     \u001b[39mreturn\u001b[39;00m MlflowClient()\u001b[39m.\u001b[39;49mget_experiment(experiment_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.8/site-packages/mlflow/tracking/client.py:428\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment\u001b[0;34m(self, experiment_id)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_experiment\u001b[39m(\u001b[39mself\u001b[39m, experiment_id: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Experiment:\n\u001b[1;32m    399\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[39m    Retrieve an experiment by experiment_id from the backend store\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39m        Lifecycle_stage: active\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mget_experiment(experiment_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py:213\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment\u001b[0;34m(self, experiment_id)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_experiment\u001b[39m(\u001b[39mself\u001b[39m, experiment_id):\n\u001b[1;32m    209\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m    :param experiment_id: The experiment ID returned from ``create_experiment``.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m    :return: :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mget_experiment(experiment_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py:406\u001b[0m, in \u001b[0;36mFileStore.get_experiment\u001b[0;34m(self, experiment_id)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39mFetch the experiment.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[39mNote: This API will search for active as well as deleted experiments.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[39m:return: A single Experiment object if it exists, otherwise raises an Exception.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m experiment_id \u001b[39m=\u001b[39m FileStore\u001b[39m.\u001b[39mDEFAULT_EXPERIMENT_ID \u001b[39mif\u001b[39;00m experiment_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m experiment_id\n\u001b[0;32m--> 406\u001b[0m experiment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_experiment(experiment_id)\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m experiment \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    409\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExperiment \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not exist.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m experiment_id,\n\u001b[1;32m    410\u001b[0m         databricks_pb2\u001b[39m.\u001b[39mRESOURCE_DOES_NOT_EXIST,\n\u001b[1;32m    411\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py:376\u001b[0m, in \u001b[0;36mFileStore._get_experiment\u001b[0;34m(self, experiment_id, view_type)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_experiment\u001b[39m(\u001b[39mself\u001b[39m, experiment_id, view_type\u001b[39m=\u001b[39mViewType\u001b[39m.\u001b[39mALL):\n\u001b[1;32m    375\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_root_dir()\n\u001b[0;32m--> 376\u001b[0m     _validate_experiment_id(experiment_id)\n\u001b[1;32m    377\u001b[0m     experiment_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_experiment_path(experiment_id, view_type)\n\u001b[1;32m    378\u001b[0m     \u001b[39mif\u001b[39;00m experiment_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.8/site-packages/mlflow/utils/validation.py:278\u001b[0m, in \u001b[0;36m_validate_experiment_id\u001b[0;34m(exp_id)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_experiment_id\u001b[39m(exp_id):\n\u001b[1;32m    277\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that `experiment_id`is a valid string or None, raise an exception if it isn't.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m     \u001b[39mif\u001b[39;00m exp_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m _EXPERIMENT_ID_REGEX\u001b[39m.\u001b[39;49mmatch(exp_id) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    280\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mInvalid experiment ID: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m exp_id, error_code\u001b[39m=\u001b[39mINVALID_PARAMETER_VALUE\n\u001b[1;32m    281\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "mlflow.get_experiment(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score,confusion_matrix,classification_report\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "# others\n",
    "import json\n",
    "import yaml\n",
    "import joblib\n",
    "import argparse\n",
    "from urllib.parse import urlparse\n",
    "import logging\n",
    "import warnings\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "\n",
    "# from params_loader import read_params\n",
    "# from models import build_and_load_models\n",
    "# from check_data_exist import check_dataset_exist\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# np.random.seed(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SKlearn model\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import  RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# from catboost import Pool, CatBoostClassifier, cv\n",
    "# import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def build_and_load_models():\n",
    "    # Models\n",
    "    Models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),                    #\n",
    "        \"Support Vector Classifier\": SVC(),                             # Ridge, SVC, LinearSVC, Passive_AC\n",
    "        \"Decision Tree\": DecisionTreeClassifier(max_depth=6),           #\n",
    "        \"KNearest\": KNeighborsClassifier(n_neighbors=5),                # doesn't have model.predict_proba so I left out.\n",
    "        \"GaussianNB\" : GaussianNB(),                                    #\n",
    "        \"LDA\" : LinearDiscriminantAnalysis(),                           # \n",
    "        \"Ridge\" : RidgeClassifier(),                                    #  \n",
    "        \"QDA\" : QuadraticDiscriminantAnalysis(),                        #\n",
    "        \"Bagging\" : BaggingClassifier(),                                #\n",
    "        \"MLP\" : MLPClassifier(),                                        #\n",
    "        \"LSVC\" : LinearSVC(),                                           #  \n",
    "        \"BernoulliNB\" : BernoulliNB(),                                  #  \n",
    "        \"Passive_AC\" : PassiveAggressiveClassifier(),                   # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #  \n",
    "        \"SGB\"     : GradientBoostingClassifier(n_estimators=100, random_state=9),\n",
    "        \"Adaboost\" : AdaBoostClassifier(n_estimators=100, random_state=9, algorithm='SAMME.R', learning_rate=0.8),\n",
    "        \"Extra_T\" : ExtraTreesClassifier(n_estimators=100, max_features=3),\n",
    "        \"R_forest\" : RandomForestClassifier(max_samples=0.9, n_estimators=100, max_features=3),\n",
    "        # \"XGB\" : xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "        }\n",
    "    return Models\n",
    " \n",
    "\n",
    "### Add lgb, catboost and Stacking and ANN\n",
    "\n",
    "\n",
    "### virtualize end result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '../../data/processed/undersampling_test.parquet'\n",
    "test_data_path  = '../../data/processed/undersampling_test.parquet'\n",
    "target = 'Class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../data/\")\n",
    "from data_function import get_feat_and_target\n",
    "def features_labels_split(train_data_path, test_data_path, target):\n",
    "    train_dataset = pd.read_parquet(train_data_path)\n",
    "    test_dataset = pd.read_parquet(test_data_path)\n",
    "    train_features,train_label = get_feat_and_target(train_dataset, target)\n",
    "    test_features,test_label = get_feat_and_target(test_dataset, target)\n",
    "    return train_features, train_label, test_features, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliiting X and y\n",
      "spliiting X and y\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_parquet(train_data_path)\n",
    "test_dataset = pd.read_parquet(test_data_path)\n",
    "train_features,train_label = get_feat_and_target(train_dataset, target)\n",
    "test_features,test_label = get_feat_and_target(test_dataset, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Models = build_and_load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': LogisticRegression(),\n",
       " 'Support Vector Classifier': SVC(),\n",
       " 'Decision Tree': DecisionTreeClassifier(max_depth=6),\n",
       " 'KNearest': KNeighborsClassifier(),\n",
       " 'GaussianNB': GaussianNB(),\n",
       " 'LDA': LinearDiscriminantAnalysis(),\n",
       " 'Ridge': RidgeClassifier(),\n",
       " 'QDA': QuadraticDiscriminantAnalysis(),\n",
       " 'Bagging': BaggingClassifier(),\n",
       " 'MLP': MLPClassifier(),\n",
       " 'LSVC': LinearSVC(),\n",
       " 'BernoulliNB': BernoulliNB(),\n",
       " 'Passive_AC': PassiveAggressiveClassifier(),\n",
       " 'SGB': GradientBoostingClassifier(random_state=9),\n",
       " 'Adaboost': AdaBoostClassifier(learning_rate=0.8, n_estimators=100, random_state=9),\n",
       " 'Extra_T': ExtraTreesClassifier(max_features=3),\n",
       " 'R_forest': RandomForestClassifier(max_features=3, max_samples=0.9)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(classifier, test_features, test_labels, avg_method):\n",
    "    \n",
    "    # make prediction\n",
    "    predictions   = classifier.predict(test_features)\n",
    "    base_score   = classifier.score(test_features,test_labels)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions, average=avg_method)\n",
    "    recall = recall_score(test_labels, predictions, average=avg_method)\n",
    "    f1score = f1_score(test_labels, predictions, average=avg_method)\n",
    "    Matrix = confusion_matrix(test_labels, predictions)\n",
    "    matrix_scores = { \n",
    "        \"true negative\"  : Matrix[0][0],\n",
    "        \"false positive\" : Matrix[0][1],\n",
    "        \"false negative\" : Matrix[1][0],\n",
    "        \"true positive \" : Matrix[1][1]\n",
    "    }\n",
    "    target_names = ['0','1']\n",
    "    print(\"Classification report\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(classification_report(test_labels, predictions,target_names=target_names),\"\\n\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(f\"{Matrix} \\n\")\n",
    "\n",
    "    print(\"Accuracy Measures\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(\"Base score: \", base_score)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 Score: \", f1score)\n",
    "    \n",
    "    return base_score,accuracy,precision,recall,f1score,matrix_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Logistic Regression\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95        98\n",
      "           1       0.97      0.94      0.95        99\n",
      "\n",
      "    accuracy                           0.95       197\n",
      "   macro avg       0.95      0.95      0.95       197\n",
      "weighted avg       0.95      0.95      0.95       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[95  3]\n",
      " [ 6 93]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9543147208121827\n",
      "Accuracy:  0.9543147208121827\n",
      "Precision:  0.9547434914811278\n",
      "Recall:  0.9543147208121827\n",
      "F1 Score:  0.9543076569885742\n",
      "2. Support Vector Classifier\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.92        98\n",
      "           1       0.99      0.83      0.90        99\n",
      "\n",
      "    accuracy                           0.91       197\n",
      "   macro avg       0.92      0.91      0.91       197\n",
      "weighted avg       0.92      0.91      0.91       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[97  1]\n",
      " [17 82]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9086294416243654\n",
      "Accuracy:  0.9086294416243654\n",
      "Precision:  0.9197624052179865\n",
      "Recall:  0.9086294416243654\n",
      "F1 Score:  0.9080610989431985\n",
      "3. Decision Tree\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        98\n",
      "           1       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00       197\n",
      "   macro avg       1.00      1.00      1.00       197\n",
      "weighted avg       1.00      1.00      1.00       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[98  0]\n",
      " [ 0 99]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  1.0\n",
      "Accuracy:  1.0\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1 Score:  1.0\n",
      "4. KNearest\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93        98\n",
      "           1       0.98      0.88      0.93        99\n",
      "\n",
      "    accuracy                           0.93       197\n",
      "   macro avg       0.93      0.93      0.93       197\n",
      "weighted avg       0.93      0.93      0.93       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[96  2]\n",
      " [12 87]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9289340101522843\n",
      "Accuracy:  0.9289340101522843\n",
      "Precision:  0.933433461979632\n",
      "Recall:  0.9289340101522843\n",
      "F1 Score:  0.9287688598970091\n",
      "5. GaussianNB\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.92        98\n",
      "           1       0.99      0.85      0.91        99\n",
      "\n",
      "    accuracy                           0.92       197\n",
      "   macro avg       0.93      0.92      0.92       197\n",
      "weighted avg       0.93      0.92      0.92       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[97  1]\n",
      " [15 84]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9187817258883249\n",
      "Accuracy:  0.9187817258883249\n",
      "Precision:  0.9274634219169902\n",
      "Recall:  0.9187817258883249\n",
      "F1 Score:  0.9183991760464946\n",
      "6. LDA\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92        98\n",
      "           1       0.96      0.88      0.92        99\n",
      "\n",
      "    accuracy                           0.92       197\n",
      "   macro avg       0.92      0.92      0.92       197\n",
      "weighted avg       0.92      0.92      0.92       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[94  4]\n",
      " [12 87]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9187817258883249\n",
      "Accuracy:  0.9187817258883249\n",
      "Precision:  0.9215939696736555\n",
      "Recall:  0.9187817258883249\n",
      "F1 Score:  0.9186643826646341\n",
      "7. Ridge\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92        98\n",
      "           1       0.96      0.87      0.91        99\n",
      "\n",
      "    accuracy                           0.91       197\n",
      "   macro avg       0.92      0.91      0.91       197\n",
      "weighted avg       0.92      0.91      0.91       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[94  4]\n",
      " [13 86]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9137055837563451\n",
      "Accuracy:  0.9137055837563451\n",
      "Precision:  0.9172256748422601\n",
      "Recall:  0.9137055837563451\n",
      "F1 Score:  0.9135452224718041\n",
      "8. QDA\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95        98\n",
      "           1       0.94      0.97      0.96        99\n",
      "\n",
      "    accuracy                           0.95       197\n",
      "   macro avg       0.95      0.95      0.95       197\n",
      "weighted avg       0.95      0.95      0.95       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[92  6]\n",
      " [ 3 96]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9543147208121827\n",
      "Accuracy:  0.9543147208121827\n",
      "Precision:  0.9547296129245179\n",
      "Recall:  0.9543147208121827\n",
      "F1 Score:  0.9543005887948021\n",
      "9. Bagging\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        98\n",
      "           1       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00       197\n",
      "   macro avg       1.00      1.00      1.00       197\n",
      "weighted avg       1.00      1.00      1.00       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[98  0]\n",
      " [ 0 99]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  1.0\n",
      "Accuracy:  1.0\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1 Score:  1.0\n",
      "10. MLP\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97        98\n",
      "           1       0.98      0.97      0.97        99\n",
      "\n",
      "    accuracy                           0.97       197\n",
      "   macro avg       0.97      0.97      0.97       197\n",
      "weighted avg       0.97      0.97      0.97       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[96  2]\n",
      " [ 3 96]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9746192893401016\n",
      "Accuracy:  0.9746192893401016\n",
      "Precision:  0.9746695170915621\n",
      "Recall:  0.9746192893401016\n",
      "F1 Score:  0.9746192893401016\n",
      "11. LSVC\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96        98\n",
      "           1       0.98      0.93      0.95        99\n",
      "\n",
      "    accuracy                           0.95       197\n",
      "   macro avg       0.96      0.95      0.95       197\n",
      "weighted avg       0.96      0.95      0.95       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[96  2]\n",
      " [ 7 92]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9543147208121827\n",
      "Accuracy:  0.9543147208121827\n",
      "Precision:  0.9554996083579661\n",
      "Recall:  0.9543147208121827\n",
      "F1 Score:  0.9542911674498813\n",
      "12. BernoulliNB\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90        98\n",
      "           1       0.98      0.81      0.88        99\n",
      "\n",
      "    accuracy                           0.89       197\n",
      "   macro avg       0.91      0.89      0.89       197\n",
      "weighted avg       0.91      0.89      0.89       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[96  2]\n",
      " [19 80]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.8934010152284264\n",
      "Accuracy:  0.8934010152284264\n",
      "Precision:  0.9055536117047331\n",
      "Recall:  0.8934010152284264\n",
      "F1 Score:  0.8926489356533515\n",
      "13. Passive_AC\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95        98\n",
      "           1       0.97      0.93      0.95        99\n",
      "\n",
      "    accuracy                           0.95       197\n",
      "   macro avg       0.95      0.95      0.95       197\n",
      "weighted avg       0.95      0.95      0.95       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[95  3]\n",
      " [ 7 92]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.949238578680203\n",
      "Accuracy:  0.949238578680203\n",
      "Precision:  0.9499908325606491\n",
      "Recall:  0.949238578680203\n",
      "F1 Score:  0.9492228792715474\n",
      "14. SGB\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        98\n",
      "           1       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00       197\n",
      "   macro avg       1.00      1.00      1.00       197\n",
      "weighted avg       1.00      1.00      1.00       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[98  0]\n",
      " [ 0 99]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  1.0\n",
      "Accuracy:  1.0\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1 Score:  1.0\n",
      "15. Adaboost\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        98\n",
      "           1       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00       197\n",
      "   macro avg       1.00      1.00      1.00       197\n",
      "weighted avg       1.00      1.00      1.00       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[98  0]\n",
      " [ 0 99]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  1.0\n",
      "Accuracy:  1.0\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1 Score:  1.0\n",
      "16. Extra_T\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        98\n",
      "           1       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00       197\n",
      "   macro avg       1.00      1.00      1.00       197\n",
      "weighted avg       1.00      1.00      1.00       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[98  0]\n",
      " [ 0 99]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  1.0\n",
      "Accuracy:  1.0\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1 Score:  1.0\n",
      "17. R_forest\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        98\n",
      "           1       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00       197\n",
      "   macro avg       1.00      1.00      1.00       197\n",
      "weighted avg       1.00      1.00      1.00       197\n",
      " \n",
      "\n",
      "Confusion Matrix\n",
      "--------------------- \n",
      "\n",
      "[[98  0]\n",
      " [ 0 99]] \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  1.0\n",
      "Accuracy:  1.0\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1 Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 1\n",
    "for Model_Name, classifier in Models.items():   \n",
    "        with mlflow.start_run():\n",
    "                print(f\"{i}. {Model_Name}\")\n",
    "                # fit the model\n",
    "                classifier.fit(train_features, train_label)\n",
    "                i = i+1\n",
    "                # Calculate the metrics\n",
    "                base_score,accuracy,precision,recall,f1score,matrix_scores = eval_metrics(classifier, \n",
    "                                                                        test_features, \n",
    "                                                                        test_label, \n",
    "                                                                        'weighted')\n",
    "                \n",
    "                mlflow.log_param(\"Models\"            , Model_Name)\n",
    "                mlflow.log_params(matrix_scores)\n",
    "                mlflow.log_metric(\"base_score\"      , base_score)\n",
    "                mlflow.log_metric(\"accuary\"         , accuracy)\n",
    "                mlflow.log_metric(\"av_precision\"    , precision)\n",
    "                mlflow.log_metric(\"recall\"          , recall)\n",
    "                mlflow.log_metric(\"f1\"              , f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m lr \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[1;32m      3\u001b[0m lr\u001b[39m.\u001b[39mfit(train_features, train_label)\n\u001b[0;32m----> 4\u001b[0m signature \u001b[39m=\u001b[39m infer_signature(test_features, lr\u001b[39m.\u001b[39mpredict(testX))\n\u001b[1;32m      5\u001b[0m base_score,accuracy,precision,recall,f1score,matrix_scores \u001b[39m=\u001b[39m eval_metrics(lr, \n\u001b[1;32m      6\u001b[0m                                                         test_features, \n\u001b[1;32m      7\u001b[0m                                                         test_label, \n\u001b[1;32m      8\u001b[0m                                                         \u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m mlflow\u001b[39m.\u001b[39mlog_model(lr, \u001b[39m\"\u001b[39m\u001b[39mlogistic regression\u001b[39m\u001b[39m\"\u001b[39m, signature\u001b[39m=\u001b[39msignature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testX' is not defined"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_features, train_label)\n",
    "    signature = infer_signature(test_features, lr.predict(testX))\n",
    "    base_score,accuracy,precision,recall,f1score,matrix_scores = eval_metrics(lr, \n",
    "                                                            test_features, \n",
    "                                                            test_label, \n",
    "                                                            'weighted')\n",
    "\n",
    "    mlflow.log_model(lr, \"logistic regression\", signature=signature)\n",
    "    mlflow.log_param(\"Models\"            , lr)\n",
    "    mlflow.log_params(matrix_scores)\n",
    "    mlflow.log_metric(\"base_score\"      , base_score)\n",
    "    mlflow.log_metric(\"accuary\"         , accuracy)\n",
    "    mlflow.log_metric(\"av_precision\"    , precision)\n",
    "    mlflow.log_metric(\"recall\"          , recall)\n",
    "    mlflow.log_metric(\"f1\"              , f1score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
